{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9ebe8f-a4ec-404d-b093-bab5ed39df44",
   "metadata": {},
   "source": [
    "# Initialize Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622230eb-ced6-4283-8fc7-35f582e19f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import models as M\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e7fda7-8e39-4234-b9a4-b75ffa674cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e10c0-14d0-489d-8c31-64c4e2d0cf2b",
   "metadata": {},
   "source": [
    "# Load Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5964661-b90b-4d32-8571-cb074ea60025",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/AnacondaLibScript/CS5246 Text Mining/Project/CNN_train/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71d4c7-c802-4093-9b60-40f8604e4d28",
   "metadata": {},
   "source": [
    "## Use only Article size of length less than 2000 and highlights size of length less than 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08096fc7-dc77-4175-8928-551cb4d2002b",
   "metadata": {},
   "source": [
    "This is too reduce the memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db78a340-c903-44a3-b88c-47c6d18f24c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22782"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_SIZE = 1700\n",
    "SUMM_SIZE = 500\n",
    "\n",
    "train = train[train['article'].apply(lambda x: len(x)<TEXT_SIZE)]\n",
    "\n",
    "train = train[train['highlights'].apply(lambda x: len(x)<SUMM_SIZE)]\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb323b0-896a-49e9-984e-65b9ca0d8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop(['index','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b327d193-4699-4687-8845-0aaff6dfca79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kabul, Afghanistan (CNN) -- China's top securi...</td>\n",
       "      <td>China's top security official visited Afghanis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN) -- Virgin, a leading branded venture cap...</td>\n",
       "      <td>The Virgin Group was founded by Richard Branso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By . Chris Pleasance . Police are hunting for ...</td>\n",
       "      <td>Two men filmed taking iPad from canoe rental o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baghdad (CNN) -- Radical Iraqi cleric Muqtada ...</td>\n",
       "      <td>Muqtada al-Sadr has been in Iran since 2007 .\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PUBLISHED: . 07:04 EST, 9 January 2014 . | . U...</td>\n",
       "      <td>Zhu Sanni, 23, had been left alone at home for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kabul, Afghanistan (CNN) -- Thousands of bottl...</td>\n",
       "      <td>Official: Bottles are almost exclusively from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(CNN) -- Tour de France race director Christia...</td>\n",
       "      <td>The 2013 Tour de France will start from the Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(CNN) -- Hundreds filed by a casket on Sunday ...</td>\n",
       "      <td>Wes Leonard collapsed after scoring a winning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Earlier this season I picked Thierry Henry as ...</td>\n",
       "      <td>Sportsmail columnist Martin Keown was honoured...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  Kabul, Afghanistan (CNN) -- China's top securi...   \n",
       "2  (CNN) -- Virgin, a leading branded venture cap...   \n",
       "3  By . Chris Pleasance . Police are hunting for ...   \n",
       "4  Baghdad (CNN) -- Radical Iraqi cleric Muqtada ...   \n",
       "5  PUBLISHED: . 07:04 EST, 9 January 2014 . | . U...   \n",
       "6  Kabul, Afghanistan (CNN) -- Thousands of bottl...   \n",
       "7  (CNN) -- Tour de France race director Christia...   \n",
       "8  (CNN) -- Hundreds filed by a casket on Sunday ...   \n",
       "9  Earlier this season I picked Thierry Henry as ...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  China's top security official visited Afghanis...  \n",
       "2  The Virgin Group was founded by Richard Branso...  \n",
       "3  Two men filmed taking iPad from canoe rental o...  \n",
       "4  Muqtada al-Sadr has been in Iran since 2007 .\\...  \n",
       "5  Zhu Sanni, 23, had been left alone at home for...  \n",
       "6  Official: Bottles are almost exclusively from ...  \n",
       "7  The 2013 Tour de France will start from the Fr...  \n",
       "8  Wes Leonard collapsed after scoring a winning ...  \n",
       "9  Sportsmail columnist Martin Keown was honoured...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0bfb0-4884-4f77-a5dd-fc8ad7d7fb20",
   "metadata": {},
   "source": [
    "To maintain consistent input shapes for the model, sequences are padded with special tokens like <PAD> so that they all have the same length. Additionally, special tokens such as <START> and <END> are added at the beginning and end of the target sequences to clearly define their boundaries. After this preprocessing step, the data is ready to be used for training and inference in the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8ed3aa-36db-41e9-9d56-e421613f079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.array(train.iloc[:, 0:1]), np.array(train.iloc[:,1:2])\n",
    "X, y = X.reshape(X.shape[0]), y.reshape(y.shape[0])\n",
    "\n",
    "START = '<start>'\n",
    "END = '<end>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "y = [f\"{START} {text} {END}\" for text in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5668d375-94df-4779-89b3-ef096f9fbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = -20\n",
    "X_valid, y_valid = X[size:], y[size:]\n",
    "X, y = X[:size], y[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71f96ef-1c79-4297-ab4c-c2c84c5cc5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22762, 22762)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac25cc4-5d09-47bb-b768-d6746cc4eeeb",
   "metadata": {},
   "source": [
    "## Set tokenizer and print vocablulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e909d6ed-73a8-4d3a-a35f-8dfe0822a1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104377, 47213)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_token, target_token = Tokenizer(), Tokenizer()\n",
    "source_token.fit_on_texts(X)\n",
    "target_token.fit_on_texts(y)\n",
    "start_id = target_token.word_index.get(START.strip('<>'))\n",
    "end_id = target_token.word_index.get(END.strip('<>'))\n",
    "pad_id = 0\n",
    "in_vocab_size, out_vocab_size = len(source_token.word_index) + 1, len(target_token.word_index) + 1\n",
    "in_vocab_size, out_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa1aaf-0d22-4668-a6a7-0b0221fbb652",
   "metadata": {},
   "source": [
    "## Convert text to sequences, padding and finalizing Encoder Input (encoder_inputs), Decoder Input (decoder_inputs) and Target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691b38d1-a72e-446a-899d-02a7520d36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = source_token.texts_to_sequences(X)\n",
    "targets = target_token.texts_to_sequences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dcacb09-ec49-462d-ae27-ab680d973475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 95)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_len = lambda x : max([len(seq) for seq in x])+1\n",
    "input_seq_len, output_seq_len = find_len(encoder_inputs), find_len(targets)\n",
    "input_seq_len, output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d93050-3956-4d3b-a0eb-e78e69b31232",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs =np.array(pad_sequences(encoder_inputs, padding='post', truncating='post', maxlen = input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d59b362c-3387-4223-89f0-6ab3eff7d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pad_sequences(targets, padding='post', truncating='post', maxlen = output_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7d12f27-f2d9-4071-a892-5317198a347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = np.array(targets[:, :-1])\n",
    "targets =  np.array(targets[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0029bd32-cbc6-4bee-b4f8-1ae855750244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104377, 47213, 331, 95)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_vocab_size, out_vocab_size, input_seq_len, output_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5accc-c7d7-4c23-adaf-2e66877d90ef",
   "metadata": {},
   "source": [
    "## Prepare Attention Mechanism, Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2973926d-f084-4b81-b7ff-35cd17e20db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(L.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = L.Dense(units)\n",
    "        self.W2 = L.Dense(units)\n",
    "        self.V = L.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query = tf.expand_dims(query, axis = 1)                \n",
    "        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(values)))  \n",
    "        attention_weight = tf.nn.softmax(score, axis = 1)     \n",
    "        context = attention_weight*values                      \n",
    "        context_vector = tf.reduce_sum(context, axis = 1)     \n",
    "        return context_vector, attention_weight\n",
    "\n",
    "class Encoder(L.Layer):\n",
    "    def __init__(self, in_vocab, embedding_dim, hidden_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = L.Embedding(in_vocab, embedding_dim)     \n",
    "        self.lstm = L.LSTM(hidden_units, return_sequences=True,return_state = True) \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embed(inputs)                             \n",
    "        enc_out, hidden_state, cell_state = self.lstm(x)    \n",
    "        return enc_out, hidden_state, cell_state\n",
    "\n",
    "class Decoder(L.Layer):\n",
    "    def __init__(self, out_vocab, embedding_dim, hidden_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = L.Embedding(out_vocab, embedding_dim)  \n",
    "        self.lstm = L.LSTM(hidden_units, return_sequences = True, return_state = True) \n",
    "        self.dense = L.Dense(out_vocab, activation='softmax')\n",
    "        self.attention = BahdanauAttention(64)\n",
    "    \n",
    "    def call(self, inputs, hidden_state, cell_state, enc_output):\n",
    "        x = self.embed(inputs)                               \n",
    "        states = [hidden_state, cell_state] \n",
    "        context, attention_weights = self.attention(query = hidden_state, values = enc_output)\n",
    "        dec_out, hidden_state, cell_state = self.lstm(x, initial_state=states)  \n",
    "        dec_out = tf.squeeze(dec_out, axis=1)                 \n",
    "      \n",
    "        inputs = tf.concat([context, dec_out], axis=-1)        \n",
    "        out = self.dense(inputs)                              \n",
    "        return out, hidden_state, cell_state "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2866e7-352f-4697-92dc-4d28c160c515",
   "metadata": {},
   "source": [
    "## Prepare Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e0ab21-7860-4474-b7e9-1b281f3afda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(M.Model):\n",
    "\n",
    "    def __init__(self, in_vocab, out_vocab, embedding_dim, hidden_units, end_token):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.in_vocab = in_vocab\n",
    "        self.out_vocab = out_vocab\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab, embedding_dim, hidden_units)\n",
    "        self.decoder = Decoder(out_vocab, embedding_dim, hidden_units)\n",
    "        self.end_token = end_token\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        (enc_inputs, dec_inputs), targets = inputs        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_out, hidden_state, cell_state = self.encoder(enc_inputs)          \n",
    "            seq_len = dec_inputs.shape[1]\n",
    "            dec_out = tf.TensorArray(tf.float32, seq_len)\n",
    "            mask = tf.TensorArray(tf.bool, size=seq_len)\n",
    "            for timestep in tf.range(seq_len):\n",
    "                timestep_input = dec_inputs[:, timestep:timestep+1]      \n",
    "                timestep_output, hidden_state, cell_state = self.decoder(timestep_input, hidden_state, cell_state, enc_out)   \n",
    "                dec_out = dec_out.write(timestep, timestep_output)\n",
    "                is_end = tf.equal(targets[:, timestep], self.end_token) \n",
    "                mask = mask.write(timestep, tf.logical_not(is_end))\n",
    "            dec_out = tf.transpose(dec_out.stack(), [1, 0, 2])\n",
    "            sequence_mask = tf.transpose(mask.stack(), [1, 0])\n",
    "            loss = self.compiled_loss(targets, dec_out, sample_weight=tf.cast(sequence_mask, tf.float32))   \n",
    "        variables = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        self.compiled_metrics.update_state(targets, dec_out) \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_inputs, dec_inputs = inputs                       \n",
    "        enc_out, hidden_state, cell_state = self.encoder(enc_inputs)   \n",
    "        seq_len = tf.shape(dec_inputs)[1]\n",
    "        dec_out = tf.TensorArray(tf.float32, seq_len)  \n",
    "        for timestep in tf.range(seq_len):\n",
    "            timestep_input = dec_inputs[:, timestep:timestep+1]       \n",
    "            timestep_output, hidden_state, cell_state = self.decoder(timestep_input, hidden_state, cell_state, enc_out)  \n",
    "            dec_out = dec_out.write(timestep, timestep_output)\n",
    "        return tf.transpose(dec_out.stack(), [1, 0, 2])\n",
    "    \n",
    "    def generate(self, enc_inputs, max_len, start, end):\n",
    "        enc_out, hidden_state, cell_state = self.encoder(enc_inputs)\n",
    "        dec_in = tf.expand_dims([start], 0)             \n",
    "        result = []\n",
    "        for _ in range(max_len): \n",
    "            prediction_logits, hidden_state, cell_state = self.decoder(dec_in, hidden_state, cell_state, enc_out) \n",
    "            prediction = tf.argmax(prediction_logits, axis=-1)        \n",
    "            if prediction == end:\n",
    "                break\n",
    "            result.append(prediction.numpy())\n",
    "            dec_in = tf.expand_dims(prediction, 0) \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Seq2Seq, self).get_config()\n",
    "        config.update({\n",
    "              'in_vocab': self.in_vocab,\n",
    "              'out_vocab': self.out_vocab,\n",
    "              'embedding_dim': self.embedding_dim,\n",
    "              'hidden_units': self.hidden_units\n",
    "          })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(\n",
    "            in_vocab=config['in_vocab'],\n",
    "            out_vocab=config['out_vocab'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            hidden_units=config['hidden_units']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a0fb2f-9654-4e58-9c65-1a5011d711f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(in_vocab=in_vocab_size, out_vocab=out_vocab_size, embedding_dim=512, hidden_units=512, end_token=end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45face21-67aa-4665-b7ec-b7a1070b6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf5c4b-7e04-4c49-8146-643c2ad69a9d",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9264f0fa-3e3b-45dc-a7aa-5173319baeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "922/922 [==============================] - 1945s 2s/step - loss: 3.2230 - accuracy: 0.6029 - val_loss: 3.0286 - val_accuracy: 0.6246\n",
      "Epoch 2/40\n",
      "922/922 [==============================] - 1942s 2s/step - loss: 2.6607 - accuracy: 0.6320 - val_loss: 2.7582 - val_accuracy: 0.6407\n",
      "Epoch 3/40\n",
      "922/922 [==============================] - 1943s 2s/step - loss: 2.3705 - accuracy: 0.6470 - val_loss: 2.6607 - val_accuracy: 0.6478\n",
      "Epoch 4/40\n",
      "922/922 [==============================] - 1946s 2s/step - loss: 2.1415 - accuracy: 0.6587 - val_loss: 2.6127 - val_accuracy: 0.6517\n",
      "Epoch 5/40\n",
      "922/922 [==============================] - 1947s 2s/step - loss: 1.9301 - accuracy: 0.6711 - val_loss: 2.6000 - val_accuracy: 0.6541\n",
      "Epoch 6/40\n",
      "922/922 [==============================] - 1948s 2s/step - loss: 1.7279 - accuracy: 0.6889 - val_loss: 2.6063 - val_accuracy: 0.6553\n",
      "Epoch 7/40\n",
      "922/922 [==============================] - 1951s 2s/step - loss: 1.5434 - accuracy: 0.7098 - val_loss: 2.6234 - val_accuracy: 0.6554\n",
      "Epoch 8/40\n",
      "922/922 [==============================] - 1945s 2s/step - loss: 1.3823 - accuracy: 0.7308 - val_loss: 2.6550 - val_accuracy: 0.6549\n",
      "Epoch 9/40\n",
      "922/922 [==============================] - 1944s 2s/step - loss: 1.2397 - accuracy: 0.7509 - val_loss: 2.6906 - val_accuracy: 0.6543\n",
      "Epoch 10/40\n",
      "922/922 [==============================] - 1947s 2s/step - loss: 1.1127 - accuracy: 0.7704 - val_loss: 2.7352 - val_accuracy: 0.6533\n",
      "Epoch 11/40\n",
      "922/922 [==============================] - 1943s 2s/step - loss: 0.9985 - accuracy: 0.7891 - val_loss: 2.7835 - val_accuracy: 0.6526\n",
      "Epoch 12/40\n",
      "922/922 [==============================] - 1944s 2s/step - loss: 0.8961 - accuracy: 0.8070 - val_loss: 2.8357 - val_accuracy: 0.6517\n",
      "Epoch 13/40\n",
      "922/922 [==============================] - 1945s 2s/step - loss: 0.8038 - accuracy: 0.8238 - val_loss: 2.8896 - val_accuracy: 0.6509\n",
      "Epoch 14/40\n",
      "922/922 [==============================] - 1940s 2s/step - loss: 0.7211 - accuracy: 0.8396 - val_loss: 2.9471 - val_accuracy: 0.6504\n",
      "Epoch 15/40\n",
      "922/922 [==============================] - 1937s 2s/step - loss: 0.6470 - accuracy: 0.8540 - val_loss: 3.0029 - val_accuracy: 0.6494\n",
      "Epoch 16/40\n",
      "922/922 [==============================] - 1935s 2s/step - loss: 0.5797 - accuracy: 0.8676 - val_loss: 3.0654 - val_accuracy: 0.6488\n",
      "Epoch 17/40\n",
      "922/922 [==============================] - 1932s 2s/step - loss: 0.5214 - accuracy: 0.8795 - val_loss: 3.1264 - val_accuracy: 0.6478\n",
      "Epoch 18/40\n",
      "922/922 [==============================] - 1935s 2s/step - loss: 0.4669 - accuracy: 0.8911 - val_loss: 3.1855 - val_accuracy: 0.6472\n",
      "Epoch 19/40\n",
      "922/922 [==============================] - 1933s 2s/step - loss: 0.4200 - accuracy: 0.9010 - val_loss: 3.2407 - val_accuracy: 0.6470\n",
      "Epoch 20/40\n",
      "922/922 [==============================] - 1945s 2s/step - loss: 0.3779 - accuracy: 0.9100 - val_loss: 3.3052 - val_accuracy: 0.6461\n",
      "Epoch 21/40\n",
      "922/922 [==============================] - 1935s 2s/step - loss: 0.3408 - accuracy: 0.9180 - val_loss: 3.3595 - val_accuracy: 0.6459\n",
      "Epoch 22/40\n",
      "922/922 [==============================] - 1937s 2s/step - loss: 0.3071 - accuracy: 0.9254 - val_loss: 3.4156 - val_accuracy: 0.6450\n",
      "Epoch 23/40\n",
      "922/922 [==============================] - 1941s 2s/step - loss: 0.2779 - accuracy: 0.9319 - val_loss: 3.4685 - val_accuracy: 0.6445\n",
      "Epoch 24/40\n",
      "922/922 [==============================] - 1939s 2s/step - loss: 0.2514 - accuracy: 0.9378 - val_loss: 3.5246 - val_accuracy: 0.6441\n",
      "Epoch 25/40\n",
      "922/922 [==============================] - 1937s 2s/step - loss: 0.2289 - accuracy: 0.9425 - val_loss: 3.5707 - val_accuracy: 0.6446\n",
      "Epoch 26/40\n",
      "922/922 [==============================] - 1938s 2s/step - loss: 0.2093 - accuracy: 0.9470 - val_loss: 3.6186 - val_accuracy: 0.6440\n",
      "Epoch 27/40\n",
      "922/922 [==============================] - 1939s 2s/step - loss: 0.1920 - accuracy: 0.9508 - val_loss: 3.6648 - val_accuracy: 0.6437\n",
      "Epoch 28/40\n",
      "922/922 [==============================] - 1938s 2s/step - loss: 0.1765 - accuracy: 0.9543 - val_loss: 3.7074 - val_accuracy: 0.6436\n",
      "Epoch 29/40\n",
      "922/922 [==============================] - 1939s 2s/step - loss: 0.1617 - accuracy: 0.9576 - val_loss: 3.7470 - val_accuracy: 0.6435\n",
      "Epoch 30/40\n",
      "922/922 [==============================] - 1942s 2s/step - loss: 0.1493 - accuracy: 0.9602 - val_loss: 3.7888 - val_accuracy: 0.6432\n",
      "Epoch 31/40\n",
      "922/922 [==============================] - 1939s 2s/step - loss: 0.1400 - accuracy: 0.9624 - val_loss: 3.8274 - val_accuracy: 0.6431\n",
      "Epoch 32/40\n",
      "922/922 [==============================] - 1936s 2s/step - loss: 0.1313 - accuracy: 0.9642 - val_loss: 3.8557 - val_accuracy: 0.6437\n",
      "Epoch 33/40\n",
      "922/922 [==============================] - 1956s 2s/step - loss: 0.1225 - accuracy: 0.9662 - val_loss: 3.8906 - val_accuracy: 0.6432\n",
      "Epoch 34/40\n",
      "922/922 [==============================] - 1934s 2s/step - loss: 0.1151 - accuracy: 0.9678 - val_loss: 3.9220 - val_accuracy: 0.6432\n",
      "Epoch 35/40\n",
      "922/922 [==============================] - 1938s 2s/step - loss: 0.1083 - accuracy: 0.9694 - val_loss: 3.9562 - val_accuracy: 0.6428\n",
      "Epoch 36/40\n",
      "922/922 [==============================] - 1939s 2s/step - loss: 0.1046 - accuracy: 0.9699 - val_loss: 3.9807 - val_accuracy: 0.6429\n",
      "Epoch 37/40\n",
      "922/922 [==============================] - 1934s 2s/step - loss: 0.0997 - accuracy: 0.9709 - val_loss: 4.0065 - val_accuracy: 0.6429\n",
      "Epoch 38/40\n",
      "922/922 [==============================] - 1939s 2s/step - loss: 0.0963 - accuracy: 0.9716 - val_loss: 4.0294 - val_accuracy: 0.6427\n",
      "Epoch 39/40\n",
      "922/922 [==============================] - 1942s 2s/step - loss: 0.0916 - accuracy: 0.9728 - val_loss: 4.0560 - val_accuracy: 0.6429\n",
      "Epoch 40/40\n",
      "922/922 [==============================] - 1941s 2s/step - loss: 0.0864 - accuracy: 0.9740 - val_loss: 4.0816 - val_accuracy: 0.6424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x236387488b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit((enc_inputs, dec_inputs), targets, batch_size=32, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f008e92-7f99-4c3c-9810-37e45700cce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/AnacondaLibScript/CS5246 Text Mining/Project/0504_2k_40Epoch\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/AnacondaLibScript/CS5246 Text Mining/Project/0504_2k_40Epoch\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('D:/AnacondaLibScript/CS5246 Text Mining/Project/0504_40Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d0c4aca-1f4c-4ae9-9a22-00fccb0aa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= tf.keras.models.load_model('D:/AnacondaLibScript/CS5246 Text Mining/Project/0504_40Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cea998-bea9-4e6e-af29-41403edc0b7c",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbf4b8cd-4801-4ab5-a2d0-66958ffa39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {v : k for k,v in target_token.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "592e0856-8e59-47ca-b225-e1ac47c1ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, enc_inputs, max_len, start, end):\n",
    "    enc_out, hidden_state, cell_state = model.encoder(enc_inputs)\n",
    "    dec_in = tf.expand_dims([start], 0)\n",
    "    dec_in = tf.cast(dec_in, tf.int32)\n",
    "    result = []\n",
    "    for _ in range(max_len): \n",
    "        prediction_logits, hidden_state, cell_state = model.decoder(dec_in, hidden_state, cell_state, enc_out)\n",
    "        prediction = tf.argmax(prediction_logits, axis=-1)\n",
    "        if prediction == end:\n",
    "            break\n",
    "        result.append(prediction.numpy())\n",
    "        dec_in = tf.expand_dims(prediction, 0)\n",
    "        dec_in = tf.cast(dec_in, tf.int32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f43268-275c-4650-a71e-57fcbbec5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(ind, model=model, source_tokenizer=source_token, target_tokenizer=target_token, source_max=input_seq_len, target_max=output_seq_len):\n",
    "    text = source_tokenizer.texts_to_sequences([X[ind]])\n",
    "    text = pad_sequences(text, maxlen=source_max, padding='post')\n",
    "    text = tf.cast(text, tf.int32)\n",
    "    model_output = generate_summary(model, text, output_seq_len, start_id, end_id)\n",
    "    output_text = []\n",
    "    for token_id in model_output:\n",
    "        token_id = token_id[0] \n",
    "        if token_id == end_id:\n",
    "            break\n",
    "        word = word_dict.get(token_id, '')\n",
    "        if word:\n",
    "            output_text.append(word)\n",
    "    print(\"Input Text\")\n",
    "    print(X[ind])\n",
    "    print('\\nInference')\n",
    "    print(' '.join(output_text))\n",
    "    print('\\nExpected Highlights')\n",
    "    print(y[ind][7:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8b04f5a-053c-4820-a150-1f6b2f40a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text\n",
      "(CNN) -- A passenger who landed at Tokyo's Narita airport over the weekend has ended up with a surprise souvenir courtesy of customs officials -- a package of cannabis. Sniffer dogs failed to find the cannabis after it had been slipped into a passenger's bag. A customs official hid the package in a suitcase belonging to a passenger arriving from Hong Kong as part of an exercise for sniffer dogs on Sunday, Reuters.com reported. However, staff then lost track of the drugs and suitcase during the exercise, a spokeswoman for Tokyo customs said. Customs regulations specify that a training suitcase be used for such exercises, but the official had used passengers' suitcases for similar purposes in the past, domestic media reported. Tokyo customs has asked anyone who finds the package to return it.\n",
      "\n",
      "Inference\n",
      "customs official slips cannabis into passenger's bag to test sniffer dogs cannabis slips through the net with officials forced to ask for its return cannabis hidden in bag of unwitting passenger from hong kong customs authorities said it was hidden from a light pole hidden in its air goods say it was not hidden inside the bag and sniffer dogs inside its bag found at the scene of nearby the suitcase it was not clear but bomb was responsible for its return authorities say return to its bonds were later found on monday said they\n",
      "\n",
      "Expected Highlights\n",
      " Customs official slips cannabis into passenger's bag to test sniffer dogs .\n",
      "Cannabis slips through the net, with officials forced to ask for its return .\n",
      "Cannabis hidden in bag of unwitting passenger from Hong Kong . \n",
      "--------------------------------------------------\n",
      "Input Text\n",
      "A 6.7-magnitude earthquake struck off Chile's Pacific coast Sunday, the U.S. Geological Survey said. The quake's epicenter was 60 kilometers (37 miles) west-northwest of Iquique, Chile, and its depth was 20 kilometers (12.4 miles), the USGS said. There were no immediate reports of damages or injuries, but local emergency officials activated a precautionary tsunami warning along the coast and urged residents to move to higher ground, regional emergency official Juan Basaez told CNN Chile. Sea level readings in the area indicate a tsunami was generated, the Pacific Tsunami Warning Center said, but there is no widespread destructive tsunami threat. The center warned of possible destruction on the coast within 200 kilometers (124 miles) from the quake's epicenter. Measuring the magnitude of earthquakes . The place where two earthquakes hit every hour .\n",
      "\n",
      "Inference\n",
      "residents evacuated from land a pilot struck another truck on friday morning injuries people were injured as the two pilots were injured authorities say there's no widespread destructive tsunami threat the quake's center was centered near the town of carterton a bridge in north east france was forced to assess passengers from the killing tsunami but was later recovered there's no damage to residents officials say authorities are investigating what are still trying to determine what caused the damage as residents recovered unhurt chile says the quake happened as a possible investigation is under investigation\n",
      "\n",
      "Expected Highlights\n",
      " A tsunami was generated, the Pacific Tsunami Warning Center says .\n",
      "Residents move to higher ground as tsunami alerts sound .\n",
      "Officials say there's no widespread destructive tsunami threat .\n",
      "The quake's epicenter was 35 kilometers from Iquique, Chile . \n",
      "--------------------------------------------------\n",
      "Input Text\n",
      "Colin Montgomerie has called on Phil Mickelson to 'stand up to the plate' and become the USA's Ryder Cup captain. But the eight-time Order of Merit champion has warned that the only way Mickelson can save face is by winning the next biennial battle against the Europeans. After the USA's heavy Ryder Cup defeat at Gleneagles last year Mickelson was scathing in his criticism of the team's captain Tom Watson, claiming former captain Paul Azinger and his pod system were far superior. VIDEO Scroll down to watch Colin Montgomery call out Phil Mickelson . Colin Montgomerie speaks to KICCA.com regarding the next Ryder Cup . Phil Mickelson publicly criticised Tom Watson's captaincy after defeat at Gleneagles last year . Watson's loss has seen the USA defeated by Europe in eight of the last 10 Ryder Cups . Montgomerie, a former European Ryder Cup-winning captain, referred to the 'uncomfortable' feeling created by Mickelson's remarks. Speaking to KICCA.com, he said: 'Phil Mickelson's comments were heard and read around the world. 'A lot of people were thinking 'stand up to the plate'. OK, if you feel that strongly about the situation become captain yourself. Montgomerie has challenged Mickelson to step up and become the next US Ryder Cup captain . Montgomerie lifts the Ryder Cup trophy as captain at Celtic Manor in 2010 . 'The trouble there, of course, is Phil would have to win the Ryder Cup, having said what he said, so it does put pressure on the whole system.' A task force has been put together to find the next Ryder Cup captain after the Americans lost eight of the last 10 events - with calls for Azinger to return and Freddie Couples also in the running.\n",
      "\n",
      "Inference\n",
      "america have lost eight of the last 10 ryder cups against europe phil mickelson publicly slated captain tom watson after defeat in 2014 colin montgomerie has urged mickelson to become the next captain after the world cup champion has seen 11 years for last year's ryder cup austria star will now miss the next five matches for next two ryder cup opener on thursday captain stuart chalmers will stay with just as joseph manager on thursday captain but finished fifth in the international league with struggling with a win in recent years last year click\n",
      "\n",
      "Expected Highlights\n",
      " America have lost eight of the last 10 Ryder Cups against Europe .\n",
      "Phil Mickelson publicly slated captain Tom Watson after defeat in 2014 .\n",
      "Colin Montgomerie has urged Mickelson to become the next captain . \n",
      "--------------------------------------------------\n",
      "Input Text\n",
      "Apple is looking into launching iPhones with a 4.7-inch and a 5.7-inch screen, Reuters reports, citing sources with knowledge of the matter. The company is also considering launching a cheaper variant of the iPhone, that would cost around $99 and come in 5-6 colors. The moves are described as being \"under discussion,\" meaning they may never actually happen. If they do, however, it would mean Apple has more or less abandoned its position as the standard-setter in the smartphone market and become just another trend-follower. Long-known for its limited palette of products, Apple always focused on the premium segment of the market, claiming the form factors of its devices, as well as the prices, are just right. For example, Samsung currently offers smartphones and phablets with more than 10 different screen sizes, while Apple has only two: the 3.5-inch iPhone 4S and the 4-inch iPhone 5. The iPhone 5 was launched last September, and Apple is widely expected to unveil a new iPhone around that time this year. One of Reuters' sources claims the company \"constantly changes product specifications almost to the final moment,\" so — as usual — no one except Apple really knows whether we'll also see a bigger or a cheaper iPhone in September as well. How do you feel about a 4.7-inch or a 5.7-inch iPhone? How about a cheaper, colorful one? Share your thoughts in the comments. This article originally appeared on Mashable.\n",
      "\n",
      "Inference\n",
      "report apple considering bigger and cheaper iphones a 4 7 inch or 7 5 inch iphone screen could be coming reuters says sources say apple's also looking at a 99 phone available in multiple colors and the report number of people were injured with a friend on the bus report says the problem was related to a minor illness that has been report since that report is safe online report says that could all have faced 25 million in 2015 many days have also yet determined why there was a number of many content sites\n",
      "\n",
      "Expected Highlights\n",
      " Report: Apple considering bigger and cheaper iPhones .\n",
      "A 4.7-inch or 5.7-inch iPhone screen could be coming, Reuters says .\n",
      "Sources say Apple's also looking at a $99 phone available in multiple colors . \n",
      "--------------------------------------------------\n",
      "Input Text\n",
      "(CNN) -- Leading Japanese golfers Ai Miyazoto, Mika Miyazoto and Momuku Ueda have joined forces in launching a website to aid the rescue effort in their native country following the recent earthquake and tsunami. Speaking ahead of this week's $1.7 million Kia Classic in California, the trio revealed they have created a fund-raising website for a relief organization to combat the devastation. All three players, who were in Japan when the disaster struck, have designed buttons with a Japanese logo which translates into: \"Never Give Up Japan\". They will be wearing the buttons on their caps during this week's LPGA event and have also distributed them to their fellow-players in an effort to promote the website further. Current world number one Yani Tseng wore one of the buttons at her press conference for the tournament. World number six, Ai Miyazoto told a news conference: \"My whole family lives in Okinawa which is way down south from Tokyo, so they are all safe. \"But I went to high school in Sendai City, so I know many people over there. I heard everyone is safe but some of my friends lost their houses and they stay together in a gymnasium,\" she added. World No.51 Ueda added:  \"I think all three of us had an idea of what we wanted to do to help. \"But the reason why all three of us got together is because by doing it together, we could have a bigger influence.\"\n",
      "\n",
      "Inference\n",
      "ai miyazoto mika miyazoto and momuku ueda launch a website to help japan the trio were all in their native country when the recent earthquake and tsunami struck they have designed a button which will raise awareness of their fundraising side the button have been handed out to all the players at this week's lpga kia classic manager confirmed steven de gea will also be a relief news for the next two years and japan has a similar to 1 0 manager this weekend the united have all scored against all it will all have\n",
      "\n",
      "Expected Highlights\n",
      " Ai Miyazoto, Mika Miyazoto and Momuku Ueda launch a website to help Japan .\n",
      "The trio were all in their native country when the recent earthquake and tsunami struck .\n",
      "They have designed a button which will raise awareness of their fundraising side .\n",
      "The button have been handed out to all the players at this week's LPGA Kia Classic . \n",
      "--------------------------------------------------\n",
      "Input Text\n",
      "By . Daily Mail Reporter . A fleeing suspected car thief bit off more than he could chew Tuesday when he crashed into an alligator that happened to be crossing the road. The unlucky reptile and 22-year-old Calvin Rodriguez collided as Rodriguez was trying to outrun the police in a stolen Honda Civic in Port St. Lucie, Florida. He wouldn't have gotten away with it, too, if it wasn't that that pesky gator. Calvin Rodriguez, 22, was arrested after crashing a stolen Honda Civic into an alligator . Officers say they had lost Rodriguez's trail as they pursued him Tuesday night. He had two pals with him in the car at the time. But they found him - and his stolen ride - smashed into a guardrail a short time later. He told officers that he lost control after crashed into the reptile. 'It’s pretty unimaginable that police officers would be at that point in time looking for these suspects and that an alligator unfortunately just happens to cross the road and assist us in catching these criminals,' Detective Keith Boham told WPTV. Police said they don't know whether the gator survived the crash (stock picture) He had in his possession a set of 'shaved keys' that he said he used to steal Honda and Acura cars from the parking lots of supermarkets around the Port St. Lucie area. Rodriguez is suspected of stealing a total of five cars before he was caught. He told officers he would drive a stolen vehicle for several days, then ditch it and steal another. He said he learned to swipe cars 'growing up in a tough town in Connecticut.' Police said they do not know whether the alligator survive the crash.\n",
      "\n",
      "Inference\n",
      "calvin rodriguez smashed his stolen car into the head of a beer in the face the buffalo is now thought to be worth more than 80 000 dangerous he was charged for driving and parked offences in his chinese village of the children said he is being held on very tall and into a real madrid he has held up his main in crime when he posted the video to his car it is currently faces charges of causing a dangerous illness that he was driving ridden by his car and is now serving a\n",
      "\n",
      "Expected Highlights\n",
      " Calvin Rodriguez smashed his stolen car into the a guardrail after hitting an alligator in the road .\n",
      "It is unknown whether the gator survived . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3200,3206):  \n",
    "    summarize(i, model) \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373858f4-0472-415b-8e4d-8182f1c2782e",
   "metadata": {},
   "source": [
    "## Metric Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35ebefaf-ff3b-4d2d-b962-29da7ab3dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ababb091-508f-4176-b52b-0e00c973a49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11490"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('D:/AnacondaLibScript/CS5246 Text Mining/Project/CNN_test/test.csv')\n",
    "test_dataset = test_dataset[test_dataset['article'].apply(lambda x: len(x)<TEXT_SIZE)]\n",
    "\n",
    "test_dataset = test_dataset[test_dataset['highlights'].apply(lambda x: len(x)<SUMM_SIZE)]\n",
    "\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdec4e47-de55-492d-b610-459c594b29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = np.array(test_dataset.iloc[:, 0:1]), np.array(test_dataset.iloc[:,1:2])\n",
    "X_test, y_test = X_test.reshape(X_test.shape[0]), y_test.reshape(y_test.shape[0])\n",
    "\n",
    "START = '<start>'\n",
    "END = '<end>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "y_test = [f\"{START} {text} {END}\" for text in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73890ee7-3823-4f16-8ac4-de4add9f405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, test_df, source_texts, target_texts, \n",
    "                    source_tokenizer, target_tokenizer, \n",
    "                    word_dict, start_id, end_id,\n",
    "                    source_max_len, target_max_len,\n",
    "                    num_samples=20):\n",
    "    \"\"\"\n",
    "    Calculate BLEU and ROUGE scores for model-generated summaries\n",
    "    \n",
    "    Args:\n",
    "        model: Your trained Seq2Seq model\n",
    "        test_df: Pandas DataFrame containing test data\n",
    "        source_texts: List of source texts (X_test)\n",
    "        target_texts: List of target texts (y_test)\n",
    "        source_tokenizer: Tokenizer for source texts\n",
    "        target_tokenizer: Tokenizer for target texts\n",
    "        word_dict: Dictionary mapping word IDs to words\n",
    "        start_id: ID of start token\n",
    "        end_id: ID of end token\n",
    "        source_max_len: Maximum length of source sequences\n",
    "        target_max_len: Maximum length of target sequences\n",
    "        num_samples: Number of samples to evaluate\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    smooth = SmoothingFunction().method1\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    all_bleu, all_rouge1, all_rouge2, all_rougeL = [], [], [], []\n",
    "    \n",
    "    # Select samples (or use all if num_samples is None)\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_df)\n",
    "    sample_indices = range(min(num_samples, len(test_df)))\n",
    "    \n",
    "    for i in tqdm(sample_indices, desc=\"Evaluating\"):\n",
    "        # Get reference and generated summaries\n",
    "        reference = target_texts[i][7:-5]  # Remove <start> and <end> tags\n",
    "        \n",
    "        # Generate model prediction\n",
    "        text = source_tokenizer.texts_to_sequences([source_texts[i]])\n",
    "        text = pad_sequences(text, maxlen=source_max_len, padding='post')\n",
    "        text = tf.cast(text, tf.int32)\n",
    "        \n",
    "        model_output = generate_summary(model, text, target_max_len, start_id, end_id)\n",
    "        \n",
    "        # Convert model output to text\n",
    "        generated = []\n",
    "        for token_id in model_output:\n",
    "            token_id = token_id[0]\n",
    "            if token_id == end_id:\n",
    "                break\n",
    "            word = word_dict.get(token_id, '')\n",
    "            if word:\n",
    "                generated.append(word)\n",
    "        generated = ' '.join(generated)\n",
    "        \n",
    "        # Tokenize for BLEU (split into words)\n",
    "        ref_tokens = [reference.split()]\n",
    "        gen_tokens = generated.split()\n",
    "        \n",
    "        # Calculate BLEU (using sentence_bleu since we're comparing one sentence at a time)\n",
    "        bleu = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smooth)\n",
    "        all_bleu.append(bleu)\n",
    "        \n",
    "        # Calculate ROUGE\n",
    "        scores = scorer.score(reference, generated)\n",
    "        all_rouge1.append(scores['rouge1'].fmeasure)\n",
    "        all_rouge2.append(scores['rouge2'].fmeasure)\n",
    "        all_rougeL.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'BLEU': np.mean(all_bleu),\n",
    "        'ROUGE-1': np.mean(all_rouge1),\n",
    "        'ROUGE-2': np.mean(all_rouge2),\n",
    "        'ROUGE-L': np.mean(all_rougeL),\n",
    "        'num_samples': len(all_bleu)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "188ee86a-e5bd-4051-9192-c9f5515c18e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491b9c0e82dd4c54afa948f03207cced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1088 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700 40\n",
      "BLEU: 0.0034\n",
      "ROUGE-1: 0.2372\n",
      "ROUGE-2: 0.0347\n",
      "ROUGE-L: 0.1362\n",
      "Evaluated on 1088 samples\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_metrics(\n",
    "    model=model,\n",
    "    test_df=test_dataset,  # or your test dataframe\n",
    "    source_texts=X_test,\n",
    "    target_texts=y_test,\n",
    "    source_tokenizer=source_token,\n",
    "    target_tokenizer=target_token,\n",
    "    word_dict=word_dict,\n",
    "    start_id=start_id,\n",
    "    end_id=end_id,\n",
    "    source_max_len=input_seq_len,\n",
    "    target_max_len=output_seq_len,\n",
    "    num_samples=None  # evaluate on 20 samples or set to None for all\n",
    ")\n",
    "print(f\"1700 40\")\n",
    "print(f\"BLEU: {metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-1: {metrics['ROUGE-1']:.4f}\")\n",
    "print(f\"ROUGE-2: {metrics['ROUGE-2']:.4f}\")\n",
    "print(f\"ROUGE-L: {metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"Evaluated on {metrics['num_samples']} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848ba7e-4229-4fb7-aade-501368bd9b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
